{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3:  Original Vocabulary\n",
    "\n",
    "* Janghyuk Boo 40008835 <br/>\n",
    "* Jixuan Li    40073785"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Part \n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we preprocessed the data.\n",
    "* We made all text lower case and then removed, stopwords and punctuation.\n",
    "* lastly we tokenized the text.\n",
    "* To filter the first occurance of character. I put it tokenized text in the dictionary to count the occurance.\n",
    "* When the count is less than 1, I removed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import  word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv (\"covid_training.tsv\", sep = '\\t')\n",
    "extracted=data[['text','q1_label']]\n",
    "# stopWords = nltk.corpus.stopwords.words('english')\n",
    "def preprocess(text): \n",
    "    tokenized = nltk.word_tokenize(text.lower())\n",
    "#     removedStopWords = [w for w in tokenized if w not in stopWords]\n",
    "#     punctual_removed = [w for w in removedStopWords if w.isalpha()]\n",
    "    punctual_removed = [w for w in tokenized if w.isalpha()]\n",
    "    return punctual_removed\n",
    "\n",
    "lower= extracted['text'].apply(lambda text: preprocess(text))\n",
    "def convert_tuple_str(tup):\n",
    "  return ' '.join(tup)\n",
    "\n",
    "def convert_tuple_str2(tup):\n",
    "    lists=list()\n",
    "    for i in tup:\n",
    "        a=' '.join(i)\n",
    "        lists.append(a)\n",
    "    return lists\n",
    "\n",
    "strings= lower.apply(lambda row: convert_tuple_str(row) )\n",
    "out = ' '.join(strings).split(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I used nltk library to count the frequency of preprocessed data.\n",
    "* first we added the smoothing amount 0.01 in all term.\n",
    "* We split the Yes/NO case from each frenquency dictionary \n",
    "* counting() is used to count the number of each case.\n",
    "* Conditional probability is calculated with baysian method. we also added smoothing factor in the denominator.\n",
    "* P(term | yes)= p(yes | term)*p(term) / p(yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "f_dist1 =FreqDist(out)\n",
    "vocab=dict(f_dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    247\n",
      "0    152\n",
      "Name: q1_label, dtype: int64\n",
      "OV vocab size 2918\n"
     ]
    }
   ],
   "source": [
    "smooth=0.01\n",
    "yes = {}\n",
    "no = {}\n",
    "factual_count=0\n",
    "not_factual_count=0\n",
    "\n",
    "for term in vocab:\n",
    "    yes[term] = smooth\n",
    "    no[term] = smooth\n",
    "    \n",
    "condition_yes = {}\n",
    "condition_no = {}\n",
    "\n",
    "\n",
    "digit=extracted['q1_label'].map(dict(yes=1, no=0))\n",
    "print(digit.value_counts())\n",
    "num_yes=digit.value_counts()[1]\n",
    "num_no=digit.value_counts()[0]\n",
    "tweet_count=num_yes+num_no\n",
    "def counting(text,target):\n",
    "    if target ==1:\n",
    "        dic= yes\n",
    "    else:\n",
    "        dic= no\n",
    "    for token in text.split(' '):\n",
    "        if token in dic:\n",
    "            dic[token]+=1\n",
    "    \n",
    "with open(\"covid_training.tsv\", encoding=\"utf8\") as file:\n",
    "    is_first_line = True\n",
    "    reader = csv.reader(file, delimiter='\\t')\n",
    "\n",
    "    for row in reader:\n",
    "        if is_first_line:\n",
    "            is_first_line = False\n",
    "            continue\n",
    "        fact = 0\n",
    "        if row[2].lower() == 'yes':\n",
    "            fact = 1\n",
    "        else:\n",
    "            fact = 0\n",
    "        \n",
    "        tweet_text = row[1].lower()\n",
    "        counting(tweet_text, fact)\n",
    "    prob_is_factual = np.log10(num_yes / tweet_count)\n",
    "    prob_is_not_factual = np.log10(num_no / tweet_count)\n",
    "\n",
    "    for term in yes:\n",
    "        condition_yes[term] = np.log10(yes[term] / (num_yes + (len(vocab) * smooth)) )#smoothed_yes\n",
    "    for term in no:\n",
    "        condition_no[term] = np.log10(no[term] / (num_no + (len(vocab) * smooth)))#smoothed_no\n",
    "\n",
    "print(\"OV vocab size\",len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Part\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to test the data by the testing set. <br/>\n",
    "\n",
    "1. We open the testing set file, and split it the data\n",
    "2. We called the `preprocess()` function we defined in part 1, to tokenize and standarize the raw tweet \n",
    "3. Loop through the token stream, calculate the final score.\n",
    "    * If the token exists in the dictionary, we simply retrieve the score from the dictionary\n",
    "    * Else the token is not in the dictionary, we calculate the smoothed 0 value. \n",
    "4. After we looped all the tokens in the stream, we can decide the whether the tweet is factual or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_result = {}\n",
    "correct_counter = 0\n",
    "with open(\"covid_test_public.tsv\", encoding='utf8') as f:\n",
    "    test_set = csv.reader(f, delimiter='\\t')\n",
    "    for line in test_set:\n",
    "        test_tweet_id = line[0]\n",
    "        test_tweet_text = preprocess(line[1])\n",
    "        test_tweet_target = line[2]\n",
    "        yes_score = prob_is_factual\n",
    "        no_score = prob_is_not_factual\n",
    "        for token in test_tweet_text:\n",
    "            cur_token = token\n",
    "            if cur_token in condition_yes:\n",
    "                yes_score += condition_yes[cur_token]\n",
    "                no_score += condition_no[cur_token]\n",
    "            else:\n",
    "                yes_score += np.log10(0.01/(num_yes + (len(vocab) * smooth)))\n",
    "                no_score += np.log10(0.01/(num_no + (len(vocab) * smooth)))\n",
    "        prediction = 'no'\n",
    "        final_score = no_score\n",
    "        if yes_score>= no_score:\n",
    "            prediction = 'yes'\n",
    "            final_score = yes_score\n",
    "        correctness = 'wrong'\n",
    "        if test_tweet_target == prediction:\n",
    "            correctness = 'correct'\n",
    "            correct_counter += 1\n",
    "        innerString = test_tweet_id+ \"  \" + prediction + \"  \" + str(final_score) + \"  \"+test_tweet_target+\"  \"+ correctness\n",
    "        test_result[test_tweet_id] = innerString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we started to output the values to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./trace_NB-BOW-OV.txt', 'w') as f:\n",
    "    for content in test_result.values():\n",
    "        f.write(content)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final time to calculate the performance of the model. \n",
    "\n",
    "1. accuracy (Acc), carriage return\n",
    "2. per-class precision (yes-P, no-P) separated by 2 spaces, then a carriage return\n",
    "3. per-class recall (yes-R, no-R) separated by 2 spaces, then a carriage return,\n",
    "4. per-class F1-measure (yes-F, no-F) separated by 2 spaces, then a carriage return,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "correct_count=0\n",
    "incorrect_count=0\n",
    "predicted_count=0\n",
    "predicted_wrong_count=0\n",
    "total_fact=0\n",
    "total_not_fact=0\n",
    "\n",
    "count=0\n",
    "\n",
    "with open(\"trace_NB-BOW-OV.txt\", encoding='utf8') as f:\n",
    "    test_set = csv.reader(f, delimiter=' ')\n",
    "    for line in test_set:\n",
    "        prediction=line[2]\n",
    "        answer=line[6]\n",
    "        correct=line[8]\n",
    "        if correct=='correct':\n",
    "            if answer == \"yes\":\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                incorrect_count += 1\n",
    "\n",
    "        if prediction == \"yes\":\n",
    "            predicted_count += 1\n",
    "        else:\n",
    "            predicted_wrong_count += 1\n",
    "        if answer == \"yes\":\n",
    "            total_fact += 1\n",
    "        else:\n",
    "            total_not_fact += 1\n",
    "\n",
    "        count+=1\n",
    "\n",
    "accuracy = (correct_count + incorrect_count) / count\n",
    "\n",
    "precision_fact = correct_count / predicted_count\n",
    "precision_not_fact = incorrect_count / predicted_wrong_count\n",
    "recall_fact = correct_count / total_fact\n",
    "recall_not_fact = incorrect_count / total_not_fact\n",
    "beta = 1\n",
    "f1_fact = ((beta**2 + 1) * precision_fact * recall_fact) / \\\n",
    "                 ((beta**2 * precision_fact) + recall_fact)\n",
    "f1_not_fact = ((beta**2 + 1) * precision_not_fact * recall_not_fact) / \\\n",
    "                     ((beta**2 * precision_not_fact) + recall_not_fact)\n",
    "\n",
    "line_to_write = \"Accuracy: {:.4}\".format(accuracy) + \"\\r\"\n",
    "line_to_write += \"Precision:  Fact:{:.4}\".format(precision_fact) + \"  \" + \"Not_Fact:{:.4}\".format(precision_not_fact) + \"\\r\"\n",
    "line_to_write += \"Recall: Fact:{:.4}\".format(recall_fact) + \"  \" + \"Not_Fact:{:.4}\".format(recall_not_fact) + \"\\r\"\n",
    "line_to_write += \"F1: Fact:{:.4}\".format(f1_fact) + \"  \" + \"Not_Fact:{:.4}\".format(f1_not_fact) + \"\\r\"\n",
    "\n",
    "# Write the line to the output file\n",
    "f = open(\"eval_NB-BOW-OV.txt\", \"a\")\n",
    "f.write(line_to_write)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
